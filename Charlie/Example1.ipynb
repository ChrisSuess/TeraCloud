{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with terraform\n",
    "\n",
    "We start with a few basics:\n",
    "\n",
    "1. `base.tf`: contains the 'boilerplate' - everything except the description of any instances\n",
    "2. `terraform_ec2_key` and `teraform_ec2_key.pub` - key pair for accessing the instances\n",
    "3. `simplejob.sh` - a simple shell script, with xbow decorations, slurm-style\n",
    "4. `input1.dat` and `input2.dat` - input files needed by simplejob.sh\n",
    "\n",
    "Our aim is to run `simplejob.sh` on a remote instance, and get the output back.\n",
    "\n",
    "One design feature: rather than stage files directly between here and the remote instance, we\n",
    "will go via an intermediate s3 bucket. That way we can keep output data safe if we delete the instance.\n",
    "\n",
    "The desired workflow is as follows:\n",
    "\n",
    "1. We run 'terraform apply' to get to a base state\n",
    "2. By parsing the terraform.tfstate file, we get a few useful and constant things like the name of the s3 bucket.\n",
    "3. We parse 'simplejob.sh' to find out what resources it is requesting.\n",
    "4. We are going to create a new job. As with a conventional job scheduler, we neeed to give the job a unique id (just a sequence number). We inspect the s3 bucket: the files for each job go into a unique \"directory\" in the bucket that has the same name as the job id. We set the new job id to one higher than the highest number we find here.\n",
    "5. We create a .tf file that contains the specification of the new instance. This includes tagging it with the job id.\n",
    "6. We run 'terraform apply' to create the new instance.\n",
    "7. While it launches, we upload the input files to the s3 bucket - not forgetting to add the job script file (simplejob.sh) as well.\n",
    "8. Once we get confirmation that the new instance is up and running, we transfer the job files to it, placing them in a subdirectory $HOME/job_id\n",
    "9. We submit the job to tsp\n",
    "10. We poll tsp evey so often, until the job is complete.\n",
    "11. We re-sync the job directory with the s3 bucket, so uploading the output files\n",
    "12. We delete the .tf file for the worker we have just finished using\n",
    "13. We run 'terraform apply' to destroy the worker\n",
    "14. We download the output files from the s3 bucket\n",
    "15. We clean out the s3 bucket\n",
    "\n",
    "The design means that it is possible to have multiple jobs running at any time, each on its own worker and each reading/writing to a unique 'directory' in the s3 bucket. \n",
    "\n",
    "It also makes it possible that there could be an independent \"reaper\" daemon process that every so often checks each current worker to see if it has finished running its job, and if so executes steps 11-13 above\n",
    "\n",
    "Once in real code, there would be possibilities to speed things up by doing stuff concurrently - e.g. while the instance boots up, files can be transferred to the s3 bucket, and while the instance is destroyed, files can be being copied out of the s3 bucket back into the local directory.\n",
    "\n",
    "Once in real code, it might look something like:\n",
    "\n",
    "1. To initialize everything, only run once:\n",
    "```\n",
    "xbow init\n",
    "```\n",
    "2. To submit a job:\n",
    "```\n",
    "xbow submit <jobfile>\n",
    "```  \n",
    "3. To check on the status of running jobs:\n",
    "```\n",
    "xbow stat\n",
    "```\n",
    "4. To look at the progress of a particular job (ls -l of the remote directory):\n",
    "```\n",
    "xbow ls <job_id>\n",
    "```\n",
    "5. To retrieve data from a finished job:\n",
    "```\n",
    "xbow retrieve <job_id>\n",
    "```    \n",
    "6. To stop a job (or clean out a finished one manually):\n",
    "```\n",
    "xbow rm <job_id>\n",
    "```   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as op\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a command to run terraform (assumed here to be using the Docker image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terraform(command):\n",
    "    \"\"\"\n",
    "    Run terraform with the given command\n",
    "    \"\"\"\n",
    "    base_command = 'docker run -i -v \"$PWD\":/wd -w /wd -v \"$HOME\"/.aws:/root/.aws hashicorp/terraform:light'\n",
    "    result = subprocess.run(base_command + ' ' + command, shell=True, capture_output=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 'terraform apply':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = terraform('apply -no-color -auto-approve')\n",
    "if result.returncode != 0:\n",
    "    print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the terraform .tfstate file, and find out how to get various useful stuff out of it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfstate():\n",
    "    with open('terraform.tfstate') as f:\n",
    "        data = json.load(f)\n",
    "    tfstate = {}\n",
    "    \n",
    "    worker_list = []\n",
    "    for resource in data['resources']:\n",
    "        for instance in resource['instances']:\n",
    "            if 'ami' in instance['attributes']:\n",
    "                worker_list.append(instance['attributes'])\n",
    "    workers = {}\n",
    "    for worker in worker_list:\n",
    "        workers[worker['tags']['JobId']] = worker\n",
    "    tfstate['workers'] = workers\n",
    "    \n",
    "    bucket = None\n",
    "    for resource in data['resources']:\n",
    "        for instance in resource['instances']:\n",
    "            if 'bucket' in instance['attributes']:\n",
    "                bucket = instance['attributes']\n",
    "    tfstate['bucket_name'] = bucket['bucket']\n",
    "    tfstate['region'] = bucket['region']\n",
    "    \n",
    "    key_name = None\n",
    "    for resource in data['resources']:\n",
    "        for instance in resource['instances']:\n",
    "            if 'key_name' in instance['attributes']:\n",
    "                key_name = instance['attributes']['key_name']\n",
    "    tfstate['key_name'] = key_name\n",
    "    \n",
    "    return tfstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfstate = get_tfstate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check existing workers (there may be none at this stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = tfstate['workers']\n",
    "for w in workers:\n",
    "    print(w, workers[w]['public_ip'], workers[w]['spot_bid_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughtongroup.charlie.xbow\n",
      "terraform_ec2_key\n",
      "eu-west-1\n"
     ]
    }
   ],
   "source": [
    "print(tfstate['bucket_name'])\n",
    "print(tfstate['key_name'])\n",
    "print(tfstate['region'])\n",
    "bucket_name = tfstate['bucket_name']\n",
    "key_name = tfstate['key_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse a job file and extract the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_script(scriptfile):\n",
    "    \"\"\"\n",
    "    Extract xbow parameters from a script file\n",
    "    \"\"\"\n",
    "    with open(scriptfile) as f:\n",
    "        lines = f.readlines()\n",
    "    result = {}\n",
    "    for line in lines:\n",
    "        if line[:6] == '#XBOW ':\n",
    "            words = line.split()\n",
    "            if len(words) != 2:\n",
    "                raise ValueError('Error cannot parse {}'.format(line))\n",
    "            paramdef = words[1]\n",
    "            if paramdef[:2] != '--':\n",
    "                raise ValueError('Error cannot parse {}'.format(line))\n",
    "            if not '=' in paramdef:\n",
    "                raise ValueError('Error cannot parse {}'.format(line))\n",
    "            try:\n",
    "                key, value = paramdef[2:].split('=')\n",
    "            except:\n",
    "                raise ValueError('Error cannot parse {}'.format(line))\n",
    "            if key in result:\n",
    "                result[key].append(value)\n",
    "            else:\n",
    "                result[key] = [value]\n",
    "    for key in result:\n",
    "        if len(result[key]) == 1:\n",
    "            result[key] = result[key][0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instance_type': 't2.small', 'job_name': 'mysim', 'upload': ['input1.dat', 'input2.dat']}\n"
     ]
    }
   ],
   "source": [
    "job_script = 'simplejob.sh'\n",
    "job_options = parse_script(job_script)\n",
    "print(job_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_job_id(bucket_name):\n",
    "    \"\"\"\n",
    "    The next job should have an id one greater than the largest so far\n",
    "    \"\"\"\n",
    "    result = subprocess.run('aws s3 ls s3://{}/'.format(bucket_name).split(), capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError('Error getting job ids from bucket')\n",
    "    job_ids = []\n",
    "    for line in result.stdout.decode().split('\\n'):\n",
    "        if 'PRE' in line:\n",
    "            job_ids.append(line.split()[1][:-1])\n",
    "    job_ids = [int(j) for j in job_ids]\n",
    "    job_ids.sort()\n",
    "    if len(job_ids) > 0:\n",
    "        next_job = job_ids[-1] + 1\n",
    "    else:\n",
    "        next_job = 0\n",
    "    return next_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "next_job = next_job_id(bucket_name)\n",
    "print(next_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instance_tf_file(instance_spec):\n",
    "    \"\"\"\n",
    "    Create a .tf file for a new instance\n",
    "    \"\"\"\n",
    "    required_keys = ['job_index', 'instance_type', 'xbow_bucket', 'key_name']\n",
    "    for key in required_keys:\n",
    "        if not key in instance_spec:\n",
    "            raise ValueError('Error - instance specification missing required key {}'.format(key))\n",
    "    \n",
    "    tf_instance_template = \"\"\"resource \"aws_spot_instance_request\" \"worker_{job_index}\" {{\n",
    "  ami           = data.aws_ami.base.id\n",
    "  instance_type = \"{instance_type}\"\n",
    "  key_name = \"{key_name}\"\n",
    "  security_groups = [\"allow_ssh\"]\n",
    "  iam_instance_profile = \"EC2InstanceRole\"\n",
    "\n",
    "  depends_on = [aws_s3_bucket.xbow_bucket]\n",
    "\n",
    "  tags = {{\n",
    "    Name = \"Worker-{job_index}\"\n",
    "    JobId = \"{job_index}\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "output \"worker_{job_index}_public_ip\" {{\n",
    "  value = aws_spot_instance_request.worker_{job_index}.public_ip\n",
    "}}\n",
    "output \"worker_{job_index}_spot_request_state\" {{\n",
    "  value = aws_spot_instance_request.worker_{job_index}.spot_request_state\n",
    "}}\n",
    "output \"worker_{job_index}_spot_bid_status\" {{\n",
    "  value = aws_spot_instance_request.worker_{job_index}.spot_bid_status\n",
    "}}\n",
    "    \"\"\"\n",
    "    tf_file = 'worker_{job_index}.tf'.format(**instance_spec)\n",
    "    with open(tf_file, 'w') as f:\n",
    "        f.write(tf_instance_template.format(**instance_spec))\n",
    "    return tf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_spec = {\n",
    "    'job_index':      next_job,\n",
    "    'instance_type':  job_options['instance_type'],\n",
    "    'xbow_bucket':    bucket_name,\n",
    "    'key_name':       key_name\n",
    "}\n",
    "tf_file = create_instance_tf_file(instance_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = terraform('apply -no-color -auto-approve')\n",
    "if result.returncode != 0:\n",
    "    print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the state of the instance - if not ready yet don't worry, we can get on with uploading data to the s3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None open\n"
     ]
    }
   ],
   "source": [
    "tfstate = get_tfstate()\n",
    "workers = tfstate['workers']\n",
    "for w in workers:\n",
    "    print(w, workers[w]['public_ip'], workers[w]['spot_request_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferring files onto the instance via an intermediate s3 bucket (for resilience/backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3Stager(object):\n",
    "    \"\"\"\n",
    "    A thing for moving files to and from instances via s3\n",
    "    \"\"\"\n",
    "    def __init__(self, bucket_id, remote_ip, key_name, remote_dir):\n",
    "        self.bucket_uri = 's3://{}'.format(bucket_id)\n",
    "        self.blob_base = op.join(self.bucket_uri, remote_dir)\n",
    "        self.remote_ip = remote_ip\n",
    "        self.key_name = key_name\n",
    "        self.remote_dir = remote_dir\n",
    "    \n",
    "    def upload(self, filenames):\n",
    "        \"\"\"\n",
    "        Upload a local file to the remote instance, via s3\n",
    "        \"\"\"\n",
    "        if not isinstance(filenames, list):\n",
    "            filenames = [filenames]\n",
    "        targetdir = self.blob_base + '/'\n",
    "        for filename in filenames:\n",
    "            result = subprocess.run(['aws', 's3', 'cp', filename, targetdir], capture_output=True)\n",
    "            if result.returncode != 0:\n",
    "                return result\n",
    "        return result\n",
    "    \n",
    "    def sync(self):\n",
    "        \"\"\"\n",
    "        Synchronise all files betweenthe s3 bucket and the instance\n",
    "        \"\"\"\n",
    "        result = subprocess.run(['ssh', '-i', self.key_name, '-o', 'StrictHostKeyChecking=no', 'ubuntu@{}'.format(self.remote_ip), \n",
    "                                  'aws', 's3', 'sync', self.blob_base, self.remote_dir], capture_output=True)\n",
    "        if result.returncode != 0:\n",
    "            return result\n",
    "        result = subprocess.run(['ssh', '-i', self.key_name, '-o', 'StrictHostKeyChecking=no', 'ubuntu@{}'.format(self.remote_ip), \n",
    "                                  'aws', 's3', 'sync', self.remote_dir, self.blob_base], capture_output=True)\n",
    "        return result\n",
    "        \n",
    "    def download(self, filenames):\n",
    "        \"\"\"\n",
    "        Download files from the S3 bucket to the current directory\n",
    "        \"\"\"\n",
    "        if not isinstance(filenames, list):\n",
    "            filenames = [filenames]\n",
    "        include_string = ' '.join(['--include \"{}\"'.format(filename) for filename in filenames])\n",
    "        \n",
    "        result = subprocess.run(['aws', 's3', 'sync', self.blob_base, '.', '--exclude', '\"*\"'] + include_string.split(), capture_output=True)\n",
    "        return result\n",
    "    \n",
    "    def ls(self):\n",
    "        \"\"\"\n",
    "        List the contents of the s3 bucket\n",
    "        \"\"\"\n",
    "        result = subprocess.run(['aws', 's3', 'ls', self.blob_base + '/'], capture_output=True)\n",
    "        return result\n",
    "    \n",
    "    def purge(self):\n",
    "        \"\"\"\n",
    "        Remove all files from the s3 bucket\n",
    "        \"\"\"\n",
    "        result = subprocess.run(['aws', 's3', 'rm', self.blob_base + '/', '--recursive'], capture_output=True)\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer files to the instance, via the s3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_job = str(next_job)\n",
    "stager = S3Stager(bucket_name, workers[next_job]['public_ip'], key_name, next_job)\n",
    "files_to_upload = job_options['upload']\n",
    "files_to_upload.append(job_script)\n",
    "result = stager.upload(files_to_upload)\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-24 17:28:11         40 input1.dat\n",
      "2020-01-24 17:28:11         40 input2.dat\n",
      "2020-01-24 17:28:12        158 simplejob.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = stager.ls()\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If neccessary, wait for confirmation that the instance is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34.253.86.87 active\n"
     ]
    }
   ],
   "source": [
    "result = terraform('refresh -no-color')\n",
    "if result.returncode != 0:\n",
    "    print(result.stdout.decode())\n",
    "tfstate = get_tfstate()\n",
    "workers = tfstate['workers']\n",
    "for w in workers:\n",
    "    print(w, workers[w]['public_ip'], workers[w]['spot_request_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now files can be transferred for s3 to the new instance (note we have to recreate the stager object, to make sure it now has a valid ip address to connect to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stager = S3Stager(bucket_name, workers[next_job]['public_ip'], key_name, next_job)\n",
    "result = stager.sync()\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a command on a remote instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remote_run(public_ip, key_name, command):\n",
    "    result = subprocess.run(['ssh', '-i', key_name, '-o', 'StrictHostKeyChecking=no', 'ubuntu@{}'.format(public_ip)] + command.split(), capture_output=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = remote_run(workers[next_job]['public_ip'], key_name, 'cd {} &&  tsp sh {}'.format(next_job, job_script))\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID   State      Output               E-Level  Times(r/u/s)   Command [run=0/1]\n",
      "0    finished   /tmp/ts-out.yOAhtp   0        0.00/0.00/0.00 sh simplejob.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = remote_run(workers[next_job]['public_ip'], key_name, 'tsp')\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = remote_run(workers[next_job]['public_ip'], key_name, 'tsp -c')\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input1.dat\n",
      "input2.dat\n",
      "output.dat\n",
      "simplejob.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = remote_run(workers[next_job]['public_ip'], key_name, 'ls {}'.format(next_job))\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-sync the worker with the s3 bucket, so it's safe to destroy it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stager.sync()\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(tf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = terraform('apply -no-color -auto-approve')\n",
    "if result.returncode != 0:\n",
    "    print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the results file from the s3 bucket, which can then be cleaned out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stager.download(\"*\")\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stager.purge()\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
