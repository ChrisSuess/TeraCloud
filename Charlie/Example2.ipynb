{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with terraform\n",
    "\n",
    "See Example 1 for the blurb. Here trying something a bit more serious - a Gromacs MD job.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as op\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a command to run terraform (assumed here to be using the Docker image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terraform(command):\n",
    "    \"\"\"\n",
    "    Run terraform with the given command\n",
    "    \"\"\"\n",
    "    base_command = 'docker run -i -v \"$PWD\":/wd -w /wd -v \"$HOME\"/.aws:/root/.aws hashicorp/terraform:light'\n",
    "    result = subprocess.run(base_command + ' ' + command, shell=True, capture_output=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 'terraform apply':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = terraform('apply -no-color -auto-approve')\n",
    "if result.returncode != 0:\n",
    "    print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the terraform .tfstate file, and find out how to get various useful stuff out of it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfstate():\n",
    "    with open('terraform.tfstate') as f:\n",
    "        data = json.load(f)\n",
    "    tfstate = {}\n",
    "    \n",
    "    worker_list = []\n",
    "    for resource in data['resources']:\n",
    "        for instance in resource['instances']:\n",
    "            if 'ami' in instance['attributes']:\n",
    "                worker_list.append(instance['attributes'])\n",
    "    workers = {}\n",
    "    for worker in worker_list:\n",
    "        workers[worker['tags']['JobId']] = worker\n",
    "    tfstate['workers'] = workers\n",
    "    \n",
    "    bucket = None\n",
    "    for resource in data['resources']:\n",
    "        for instance in resource['instances']:\n",
    "            if 'bucket' in instance['attributes']:\n",
    "                bucket = instance['attributes']\n",
    "    tfstate['bucket_name'] = bucket['bucket']\n",
    "    tfstate['region'] = bucket['region']\n",
    "    \n",
    "    key_name = None\n",
    "    for resource in data['resources']:\n",
    "        for instance in resource['instances']:\n",
    "            if 'key_name' in instance['attributes']:\n",
    "                key_name = instance['attributes']['key_name']\n",
    "    tfstate['key_name'] = key_name\n",
    "    \n",
    "    return tfstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfstate = get_tfstate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check existing workers (there may be none at this stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = tfstate['workers']\n",
    "for w in workers:\n",
    "    print(w, workers[w]['public_ip'], workers[w]['spot_bid_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughtongroup.charlie.xbow\n",
      "terraform_ec2_key\n",
      "eu-west-1\n"
     ]
    }
   ],
   "source": [
    "print(tfstate['bucket_name'])\n",
    "print(tfstate['key_name'])\n",
    "print(tfstate['region'])\n",
    "bucket_name = tfstate['bucket_name']\n",
    "key_name = tfstate['key_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse a job file and extract the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_script(scriptfile):\n",
    "    \"\"\"\n",
    "    Extract xbow parameters from a script file\n",
    "    \"\"\"\n",
    "    with open(scriptfile) as f:\n",
    "        lines = f.readlines()\n",
    "    result = {}\n",
    "    for line in lines:\n",
    "        if line[:6] == '#XBOW ':\n",
    "            words = line.split()\n",
    "            if len(words) != 2:\n",
    "                raise ValueError('Error cannot parse {}'.format(line))\n",
    "            paramdef = words[1]\n",
    "            if paramdef[:2] != '--':\n",
    "                raise ValueError('Error cannot parse {}'.format(line))\n",
    "            if not '=' in paramdef:\n",
    "                raise ValueError('Error cannot parse {}'.format(line))\n",
    "            try:\n",
    "                key, value = paramdef[2:].split('=')\n",
    "            except:\n",
    "                raise ValueError('Error cannot parse {}'.format(line))\n",
    "            if key in result:\n",
    "                result[key].append(value)\n",
    "            else:\n",
    "                result[key] = [value]\n",
    "    for key in result:\n",
    "        if len(result[key]) == 1:\n",
    "            result[key] = result[key][0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instance_type': 'p2.xlarge', 'upload': 'bpti-md.tpr'}\n"
     ]
    }
   ],
   "source": [
    "job_script = 'runjob.sh'\n",
    "job_options = parse_script(job_script)\n",
    "print(job_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_job_id(bucket_name):\n",
    "    \"\"\"\n",
    "    The next job should have an id one greater than the largest so far\n",
    "    \"\"\"\n",
    "    result = subprocess.run('aws s3 ls s3://{}/'.format(bucket_name).split(), capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError('Error getting job ids from bucket')\n",
    "    job_ids = []\n",
    "    for line in result.stdout.decode().split('\\n'):\n",
    "        if 'PRE' in line:\n",
    "            job_ids.append(line.split()[1][:-1])\n",
    "    job_ids = [int(j) for j in job_ids]\n",
    "    job_ids.sort()\n",
    "    if len(job_ids) > 0:\n",
    "        next_job = job_ids[-1] + 1\n",
    "    else:\n",
    "        next_job = 0\n",
    "    return next_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "next_job = next_job_id(bucket_name)\n",
    "print(next_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instance_tf_file(instance_spec):\n",
    "    \"\"\"\n",
    "    Create a .tf file for a new instance\n",
    "    \"\"\"\n",
    "    required_keys = ['job_index', 'instance_type', 'xbow_bucket', 'key_name']\n",
    "    for key in required_keys:\n",
    "        if not key in instance_spec:\n",
    "            raise ValueError('Error - instance specification missing required key {}'.format(key))\n",
    "    \n",
    "    tf_instance_template = \"\"\"resource \"aws_spot_instance_request\" \"worker_{job_index}\" {{\n",
    "  ami           = data.aws_ami.base.id\n",
    "  instance_type = \"{instance_type}\"\n",
    "  key_name = \"{key_name}\"\n",
    "  security_groups = [\"allow_ssh\"]\n",
    "  iam_instance_profile = \"EC2InstanceRole\"\n",
    "\n",
    "  depends_on = [aws_s3_bucket.xbow_bucket]\n",
    "\n",
    "  tags = {{\n",
    "    Name = \"Worker-{job_index}\"\n",
    "    JobId = \"{job_index}\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "output \"worker_{job_index}_public_ip\" {{\n",
    "  value = aws_spot_instance_request.worker_{job_index}.public_ip\n",
    "}}\n",
    "output \"worker_{job_index}_spot_request_state\" {{\n",
    "  value = aws_spot_instance_request.worker_{job_index}.spot_request_state\n",
    "}}\n",
    "output \"worker_{job_index}_spot_bid_status\" {{\n",
    "  value = aws_spot_instance_request.worker_{job_index}.spot_bid_status\n",
    "}}\n",
    "    \"\"\"\n",
    "    tf_file = 'worker_{job_index}.tf'.format(**instance_spec)\n",
    "    with open(tf_file, 'w') as f:\n",
    "        f.write(tf_instance_template.format(**instance_spec))\n",
    "    return tf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_spec = {\n",
    "    'job_index':      next_job,\n",
    "    'instance_type':  job_options['instance_type'],\n",
    "    'xbow_bucket':    bucket_name,\n",
    "    'key_name':       key_name\n",
    "}\n",
    "tf_file = create_instance_tf_file(instance_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = terraform('apply -no-color -auto-approve')\n",
    "if result.returncode != 0:\n",
    "    print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the state of the instance - if not ready yet don't worry, we can get on with uploading data to the s3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None open\n"
     ]
    }
   ],
   "source": [
    "tfstate = get_tfstate()\n",
    "workers = tfstate['workers']\n",
    "for w in workers:\n",
    "    print(w, workers[w]['public_ip'], workers[w]['spot_request_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferring files onto the instance via an intermediate s3 bucket (for resilience/backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3Stager(object):\n",
    "    \"\"\"\n",
    "    A thing for moving files to and from instances via s3\n",
    "    \"\"\"\n",
    "    def __init__(self, bucket_id, remote_ip, key_name, remote_dir):\n",
    "        self.bucket_uri = 's3://{}'.format(bucket_id)\n",
    "        self.blob_base = op.join(self.bucket_uri, remote_dir)\n",
    "        self.remote_ip = remote_ip\n",
    "        self.key_name = key_name\n",
    "        self.remote_dir = remote_dir\n",
    "    \n",
    "    def upload(self, filenames):\n",
    "        \"\"\"\n",
    "        Upload a local file to the remote instance, via s3\n",
    "        \"\"\"\n",
    "        if not isinstance(filenames, list):\n",
    "            filenames = [filenames]\n",
    "        targetdir = self.blob_base + '/'\n",
    "        for filename in filenames:\n",
    "            result = subprocess.run(['aws', 's3', 'cp', filename, targetdir], capture_output=True)\n",
    "            if result.returncode != 0:\n",
    "                return result\n",
    "        return result\n",
    "    \n",
    "    def sync(self):\n",
    "        \"\"\"\n",
    "        Synchronise all files betweenthe s3 bucket and the instance\n",
    "        \"\"\"\n",
    "        result = subprocess.run(['ssh', '-i', self.key_name, '-o', 'StrictHostKeyChecking=no', 'ubuntu@{}'.format(self.remote_ip), \n",
    "                                  'aws', 's3', 'sync', self.blob_base, self.remote_dir], capture_output=True)\n",
    "        if result.returncode != 0:\n",
    "            return result\n",
    "        result = subprocess.run(['ssh', '-i', self.key_name, '-o', 'StrictHostKeyChecking=no', 'ubuntu@{}'.format(self.remote_ip), \n",
    "                                  'aws', 's3', 'sync', self.remote_dir, self.blob_base], capture_output=True)\n",
    "        return result\n",
    "        \n",
    "    def download(self, filenames):\n",
    "        \"\"\"\n",
    "        Download files from the S3 bucket to the current directory\n",
    "        \"\"\"\n",
    "        if not isinstance(filenames, list):\n",
    "            filenames = [filenames]\n",
    "        include_string = ' '.join(['--include \"{}\"'.format(filename) for filename in filenames])\n",
    "        \n",
    "        result = subprocess.run(['aws', 's3', 'sync', self.blob_base, '.', '--exclude', '\"*\"'] + include_string.split(), capture_output=True)\n",
    "        return result\n",
    "    \n",
    "    def ls(self):\n",
    "        \"\"\"\n",
    "        List the contents of the s3 bucket\n",
    "        \"\"\"\n",
    "        result = subprocess.run(['aws', 's3', 'ls', self.blob_base + '/'], capture_output=True)\n",
    "        return result\n",
    "    \n",
    "    def purge(self):\n",
    "        \"\"\"\n",
    "        Remove all files from the s3 bucket\n",
    "        \"\"\"\n",
    "        result = subprocess.run(['aws', 's3', 'rm', self.blob_base + '/', '--recursive'], capture_output=True)\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer files to the instance, via the s3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_job = str(next_job)\n",
    "stager = S3Stager(bucket_name, workers[next_job]['public_ip'], key_name, next_job)\n",
    "files_to_upload = job_options['upload']\n",
    "if not isinstance(files_to_upload, list):\n",
    "    files_to_upload = [files_to_upload]\n",
    "files_to_upload.append(job_script)\n",
    "result = stager.upload(files_to_upload)\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-24 18:15:32     799108 bpti-md.tpr\n",
      "2020-01-24 18:15:33        165 runjob.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = stager.ls()\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If neccessary, wait for confirmation that the instance is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 52.208.232.28 active\n"
     ]
    }
   ],
   "source": [
    "result = terraform('refresh -no-color')\n",
    "if result.returncode != 0:\n",
    "    print(result.stdout.decode())\n",
    "tfstate = get_tfstate()\n",
    "workers = tfstate['workers']\n",
    "for w in workers:\n",
    "    print(w, workers[w]['public_ip'], workers[w]['spot_request_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now files can be transferred for s3 to the new instance (note we have to recreate the stager object, to make sure it now has a valid ip address to connect to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['ssh', '-i', 'terraform_ec2_key', '-o', 'StrictHostKeyChecking=no', 'ubuntu@52.208.232.28', 'aws', 's3', 'sync', '0', 's3://laughtongroup.charlie.xbow/0'], returncode=1, stdout=b'Completed 1 file(s) with ~0 file(s) remaining (calculating...)\\r', stderr=b'upload failed: 0/dask-worker-space/global.lock to s3://laughtongroup.charlie.xbow/0/dask-worker-space/global.lock seek() takes 2 positional arguments but 3 were given\\nupload failed: 0/dask-worker-space/purge.lock to s3://laughtongroup.charlie.xbow/0/dask-worker-space/purge.lock seek() takes 2 positional arguments but 3 were given\\n')\n"
     ]
    }
   ],
   "source": [
    "stager = S3Stager(bucket_name, workers[next_job]['public_ip'], key_name, next_job)\n",
    "result = stager.sync()\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a command on a remote instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remote_run(public_ip, key_name, command):\n",
    "    result = subprocess.run(['ssh', '-i', key_name, '-o', 'StrictHostKeyChecking=no', 'ubuntu@{}'.format(public_ip)] + command.split(), capture_output=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = remote_run(workers[next_job]['public_ip'], key_name, 'cd {} &&  tsp sh {}'.format(next_job, job_script))\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID   State      Output               E-Level  Times(r/u/s)   Command [run=0/1]\n",
      "0    finished   /tmp/ts-out.b05bWj   1        25.23/6.54/0.65 sh runjob.sh\n",
      "1    finished   /tmp/ts-out.ZpOBrj   127      1.53/1.39/0.10 sh runjob.sh\n",
      "2    finished   /tmp/ts-out.LXpEl0   0        56.42/1.50/0.17 sh runjob.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = remote_run(workers[next_job]['public_ip'], key_name, 'tsp')\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully installed gromacs 2019-cuda\n",
      "Unable to find image 'claughton/gromacs:2019-cuda' locally\n",
      "2019-cuda: Pulling from claughton/gromacs\n",
      "18d680d61657: Pulling fs layer\n",
      "0addb6fece63: Pulling fs layer\n",
      "78e58219b215: Pulling fs layer\n",
      "eb6959a66df2: Pulling fs layer\n",
      "c6aa9245dd49: Pulling fs layer\n",
      "f0233a859d9b: Pulling fs layer\n",
      "b4c3e8ab5f01: Pulling fs layer\n",
      "04f88a0781f7: Pulling fs layer\n",
      "57f0a9a9301a: Pulling fs layer\n",
      "f0233a859d9b: Waiting\n",
      "04f88a0781f7: Waiting\n",
      "57f0a9a9301a: Waiting\n",
      "b4c3e8ab5f01: Waiting\n",
      "eb6959a66df2: Waiting\n",
      "c6aa9245dd49: Waiting\n",
      "0addb6fece63: Verifying Checksum\n",
      "0addb6fece63: Download complete\n",
      "78e58219b215: Verifying Checksum\n",
      "78e58219b215: Download complete\n",
      "18d680d61657: Verifying Checksum\n",
      "18d680d61657: Download complete\n",
      "eb6959a66df2: Verifying Checksum\n",
      "eb6959a66df2: Download complete\n",
      "c6aa9245dd49: Verifying Checksum\n",
      "c6aa9245dd49: Download complete\n",
      "b4c3e8ab5f01: Verifying Checksum\n",
      "b4c3e8ab5f01: Download complete\n",
      "18d680d61657: Pull complete\n",
      "0addb6fece63: Pull complete\n",
      "78e58219b215: Pull complete\n",
      "eb6959a66df2: Pull complete\n",
      "c6aa9245dd49: Pull complete\n",
      "57f0a9a9301a: Verifying Checksum\n",
      "57f0a9a9301a: Download complete\n",
      "04f88a0781f7: Verifying Checksum\n",
      "04f88a0781f7: Download complete\n",
      "f0233a859d9b: Verifying Checksum\n",
      "f0233a859d9b: Download complete\n",
      "f0233a859d9b: Pull complete\n",
      "b4c3e8ab5f01: Pull complete\n",
      "04f88a0781f7: Pull complete\n",
      "57f0a9a9301a: Pull complete\n",
      "Digest: sha256:4af2135d352eb5e23326afe23295fea5edeaff647c566baf760b8ee2e49d843c\n",
      "Status: Downloaded newer image for claughton/gromacs:2019-cuda\n",
      "                       :-) GROMACS - gmx mdrun, 2019 (-:\n",
      "\n",
      "                            GROMACS is written by:\n",
      "     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen\n",
      "    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    \n",
      " Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     \n",
      "  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  \n",
      "  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis\n",
      "    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    \n",
      "  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   \n",
      "    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   \n",
      "    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  \n",
      "   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen \n",
      " Christian Wennberg    Maarten Wolf   \n",
      "                           and the project leaders:\n",
      "        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel\n",
      "\n",
      "Copyright (c) 1991-2000, University of Groningen, The Netherlands.\n",
      "Copyright (c) 2001-2018, The GROMACS development team at\n",
      "Uppsala University, Stockholm University and\n",
      "the Royal Institute of Technology, Sweden.\n",
      "check out http://www.gromacs.org for more information.\n",
      "\n",
      "GROMACS is free software; you can redistribute it and/or modify it\n",
      "under the terms of the GNU Lesser General Public License\n",
      "as published by the Free Software Foundation; either version 2.1\n",
      "of the License, or (at your option) any later version.\n",
      "\n",
      "GROMACS:      gmx mdrun, version 2019\n",
      "Executable:   /opt/gromacs/sse2/bin/gmx\n",
      "Data prefix:  /opt/gromacs/sse2\n",
      "Working dir:  /wd\n",
      "Command line:\n",
      "  gmx mdrun -deffnm bpti-md\n",
      "\n",
      "Compiled SIMD: SSE2, but for this host/run AVX2_256 might be better (see log).\n",
      "The current CPU can measure timings more accurately than the code in\n",
      "gmx mdrun was configured to use. This might affect your simulation\n",
      "speed as accurate timings are needed for load-balancing.\n",
      "Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.\n",
      "Reading file bpti-md.tpr, VERSION 2018.1 (single precision)\n",
      "Note: file tpx version 112, software tpx version 116\n",
      "\n",
      "Using 1 MPI thread\n",
      "Using 4 OpenMP threads \n",
      "\n",
      "1 GPU auto-selected for this run.\n",
      "Mapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:\n",
      "  PP:0,PME:0\n",
      "PP tasks will do (non-perturbed) short-ranged interactions on the GPU\n",
      "PME tasks will do all aspects on the GPU\n",
      "starting mdrun 'Protein in water'\n",
      "200 steps,      0.4 ps.\n",
      "\n",
      "Writing final coordinates.\n",
      "\n",
      "NOTE: 28 % of the run time was spent in pair search,\n",
      "      you might want to increase nstlist (this has no effect on accuracy)\n",
      "\n",
      "               Core t (s)   Wall t (s)        (%)\n",
      "       Time:       10.707        2.677      400.0\n",
      "                 (ns/day)    (hour/ns)\n",
      "Performance:       12.975        1.850\n",
      "\n",
      "GROMACS reminds you: \"Kissing You is Like Kissing Gravel\" (Throwing Muses)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = remote_run(workers[next_job]['public_ip'], key_name, 'tsp -c')\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpti-md.cpt\n",
      "bpti-md.edr\n",
      "bpti-md.gro\n",
      "bpti-md.log\n",
      "bpti-md.tpr\n",
      "dask-worker-space\n",
      "runjob.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = remote_run(workers[next_job]['public_ip'], key_name, 'ls {}'.format(next_job))\n",
    "print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-sync the worker with the s3 bucket, so it's safe to destroy it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['ssh', '-i', 'terraform_ec2_key', '-o', 'StrictHostKeyChecking=no', 'ubuntu@52.208.232.28', 'aws', 's3', 'sync', '0', 's3://laughtongroup.charlie.xbow/0'], returncode=1, stdout=b'Completed 0 Bytes/1.8 MiB (0 Bytes/s) with 5 file(s) remaining\\rCompleted 0 Bytes/1.8 MiB (0 Bytes/s) with 4 file(s) remaining\\rCompleted 1.5 KiB/1.8 MiB (67.9 KiB/s) with 4 file(s) remaining\\rupload: 0/bpti-md.edr to s3://laughtongroup.charlie.xbow/0/bpti-md.edr\\nCompleted 1.5 KiB/1.8 MiB (67.9 KiB/s) with 3 file(s) remaining\\rCompleted 257.5 KiB/1.8 MiB (6.0 MiB/s) with 3 file(s) remaining\\rCompleted 513.5 KiB/1.8 MiB (10.3 MiB/s) with 3 file(s) remaining\\rCompleted 769.5 KiB/1.8 MiB (15.0 MiB/s) with 3 file(s) remaining\\rCompleted 1.0 MiB/1.8 MiB (19.5 MiB/s) with 3 file(s) remaining  \\rCompleted 1.3 MiB/1.8 MiB (23.9 MiB/s) with 3 file(s) remaining  \\rCompleted 1.5 MiB/1.8 MiB (28.1 MiB/s) with 3 file(s) remaining  \\rCompleted 1.5 MiB/1.8 MiB (20.5 MiB/s) with 3 file(s) remaining  \\rupload: 0/bpti-md.log to s3://laughtongroup.charlie.xbow/0/bpti-md.log\\nCompleted 1.5 MiB/1.8 MiB (20.5 MiB/s) with 2 file(s) remaining\\rCompleted 1.7 MiB/1.8 MiB (18.8 MiB/s) with 2 file(s) remaining\\rupload: 0/bpti-md.cpt to s3://laughtongroup.charlie.xbow/0/bpti-md.cpt\\nCompleted 1.7 MiB/1.8 MiB (18.8 MiB/s) with 1 file(s) remaining\\rCompleted 1.8 MiB/1.8 MiB (17.0 MiB/s) with 1 file(s) remaining\\rupload: 0/bpti-md.gro to s3://laughtongroup.charlie.xbow/0/bpti-md.gro\\n', stderr=b'upload failed: 0/dask-worker-space/global.lock to s3://laughtongroup.charlie.xbow/0/dask-worker-space/global.lock seek() takes 2 positional arguments but 3 were given\\nupload failed: 0/dask-worker-space/purge.lock to s3://laughtongroup.charlie.xbow/0/dask-worker-space/purge.lock seek() takes 2 positional arguments but 3 were given\\n')\n"
     ]
    }
   ],
   "source": [
    "result = stager.sync()\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(tf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = terraform('apply -no-color -auto-approve')\n",
    "if result.returncode != 0:\n",
    "    print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the results file from the s3 bucket, which can then be cleaned out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stager.download(\"*\")\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stager.purge()\n",
    "if result.returncode != 0:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
